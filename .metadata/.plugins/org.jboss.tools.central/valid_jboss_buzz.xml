<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title type="html">Kogito Task Management API</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/pYVrUNudfSI/kogito-task-management-api.html" /><author><name>Francisco Javier Tirado Sarti</name></author><id>https://blog.kie.org/2021/06/kogito-task-management-api.html</id><updated>2021-06-08T10:46:19Z</updated><content type="html">Kogito Task management API In a previous we discussed the task process API, which dealt with tasks from a process perspective. As explained in there, the task model depends on data defined by users. But, regardless of the task related information provided by users through BPMN processes, every existing task in Kogito has some predefined fields, mainly related to security policies, which are handled by Tasks Management API. Since Task Management API is a security-sensitive API, it is often referred as Admin API, but that name might cause confusion and should be avoided. Reason is that  this API is not intended for an administrator to change all task parameters arbitrarily, but for any user with enough privileges (in future, when Kogito defines a global authentication policy, API endpoint will be available only to users with certain privileges, administrators will be of course included among them)  to modify those task parameters not exposed by  Task Process API.  One important implication of this line of thought is that users with administrative privileges must use Task process API, as any other regular user, to perform phase transitions, change output parameters or add comments and attachments. TASK MANAGEMENT INFORMATION The information that can be consulted or modified by Task Management API consist of: * Description. Human readable description of the task     * Priority. A string field that indicates task priority.         * Potential Owners. The list of users and the list of groups which can claim ownership     of the task     * Administrators. The list of users and the list of groups which are task administrators, meaning that they can perform API process calls without being the current owners.         * Input parameters. A list of key value pairs. Although it includes the input model, which as you recall is automatically calculated during process execution, from a management perspective you will probably be more interested in interacting with additional task metadata information, also incorporated here. Fields like name, actorId or skippable, which might be useful to to change in certain specific scenarios, but cannot be modified using Task process API, because they are not part of the task model.     Some readers might be wondering, what about the current task owner? Following the rationale previously exposed, since the current task owner is expected to be established by using Claim o Release transitions, it was decided to not include it in the list of parameters that can be modified by Task Management API. Again, the administrative user can use the regular Task Process API to do so. TASK MANAGEMENT OPERATIONS Task management API consists of just one resource, which, in an outburst of originality, is called task. Endpoint URI match this template http://&lt;host&gt;:&lt;port&gt;/management/processes/&lt;process id&gt;/instances/&lt;process instance id&gt;/tasks/&lt;task Id&gt; Task management endpoint supports following HTTP methods:  * GET is used to consult task information. * PUT is used to replace whole task information, meaning you need to pass the whole task. Therefore, if you did not include a field in the request, this field will be set to null.   * PATCH is used to replace only certain fields, meaning that you pass only those fields you want to modify. Since usually you only need to modify a subset of all the fields, this will be the method you will be employing most.  RETRIEVING TASK INFORMATION We will illustrate management API usage by employing the same example from previous   Since you already have the proper process instance Id and task Id, invoking GET http://localhost:8080/management/processes/approvals/instances/&lt;process instance id&gt;/tasks/&lt;task Id&gt; will return as response a JSON similar to this one { "description": null, "priority": null, "potentialUsers": [ "manager" ], "potentialGroups": [ "managers" ], "excludedUsers": [], "adminUsers": [], "adminGroups": [], "inputParams": { "TaskName": "firstLineApproval", "NodeName": "First Line Approval", "Skippable": "true", "ActorId": "manager", "traveller": { "firstName": "John", "lastName": "Doe", "email": "jon.doe@example.com", "nationality": "American", "address": { "street": "main street", "city": "Boston", "zipCode": "10005", "country": "US" } }, "GroupId": "managers" } } Note that inputParams field contains a traveller instance, which is the model defined for firstLineApproval task, but also includes NodeName, Skippable and ActorId, which are not part of it.   UPDATING TASK INFORMATION Let’s assume you want to add a description, change skippable input parameter to false and add user menganito to the list of potential users for that task instance, you should invoke  PATCH http://localhost:8080/management/processes/approvals/instances/&lt;process instance id&gt;/tasks/&lt;task Id&gt; providing as body { "inputParams" : {"Skippable":false}, "description" : " Real Betis Balompie is the best soccer team in the world", "potentialUsers":["manager","meganito"] } Please note that when using PATCH you only need to specify those fields that you want to modify, but if the field data type is a JSON collection, you need to specify all the items in the collection. That’s why we only provide skippable as inputParams value (because input parameters are merged) but the complete list for potentialUsers fields (because collections are not merged).  The response of previous invocation will consist on the whole resource properly modified { "description": " Real Betis Balompie is the best soccer team in the world", "priority": null, "potentialUsers": [ "manager", “menganito” ], "potentialGroups": [ "managers" ], "excludedUsers": [], "adminUsers": [], "adminGroups": [], "inputParams": { "TaskName": "firstLineApproval", "NodeName": "First Line Approval", "Skippable": false, "ActorId": "manager", "traveller": { "firstName": "John", "lastName": "Doe", "email": "jon.doe@example.com", "nationality": "American", "address": { "street": "main street", "city": "Boston", "zipCode": "10005", "country": "US" } }, "GroupId": "managers", } } CONCLUSION  In this post we discussed the Task Management API, a brief (just one resource) but powerful one, which is intended to interact with task information that is not part of the process model. Next post, the last of this series, will describe how to establish task deadlines, whose main purpose is to make sure that a task does not fall into oblivion.  The post appeared first on .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/pYVrUNudfSI" height="1" width="1" alt=""/&gt;</content><dc:creator>Francisco Javier Tirado Sarti</dc:creator><feedburner:origLink>https://blog.kie.org/2021/06/kogito-task-management-api.html</feedburner:origLink></entry><entry><title>How to size your projects for Red Hat's single sign-on technology</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/8lHuPmBeSk4/how-size-your-projects-red-hats-single-sign-technology" /><author><name>Nicolas Massé</name></author><id>8de11e09-556e-4809-b65c-ef2af721ed32</id><updated>2021-06-07T07:00:00Z</updated><published>2021-06-07T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://access.redhat.com/products/red-hat-single-sign-on"&gt;Red Hat's single sign-on (SSO) technology&lt;/a&gt; is an identity and access management tool included in the Red Hat Middleware Core Services Collection that's based on the well-known &lt;a href="https://www.keycloak.org/"&gt;Keycloak&lt;/a&gt; open source project. As with other Red Hat products, users have to acquire subscriptions, which are priced according to the number of cores or vCPU used to deploy the product.&lt;/p&gt; &lt;p&gt;This presents an interesting problem for pre-sales engineers like me. To help my customers acquire the correct number of subscriptions, I need to sketch the target architecture and count how many cores they need. This would not be a problem if off-the-shelf performance benchmarks were available; however, they are not.&lt;/p&gt; &lt;p&gt;This article will help colleagues and customers estimate their SSO projects more precisely. We will examine the performance benchmarks I ran, how I designed them, the results I gathered, and how I drew conclusions to size my SSO project.&lt;/p&gt; &lt;h2&gt;Performance benchmarks: Framing the problem&lt;/h2&gt; &lt;p&gt;Performance benchmarking is a broad topic, and if you don't correctly frame the problem, it is easy to answer a question that has not been asked. So, for my situation, I wanted to answer the following questions:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Which architectural choices have the most impact on the number of cores used?&lt;/li&gt; &lt;li&gt;Which part of the user’s session has the most impact on the number of cores used? Opening the session? Renewing the token? Validating the token?&lt;/li&gt; &lt;li&gt;How many transactions per second (TPS) can we expect per core from typical server-grade hardware?&lt;/li&gt; &lt;/ul&gt;&lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: Between a low-end server CPU and a high-end server CPU, there can be a significant gap in terms of single-thread performance. Therefore, I am not interested in precise figures, but rather the order of magnitude (e.g., 1, 10, 100, or 1,000 TPS?).&lt;/p&gt; &lt;h2&gt;Planning the performance assessment&lt;/h2&gt; &lt;p&gt;In the Keycloak repository, there is a &lt;a href="https://github.com/keycloak/keycloak/tree/master/testsuite/performance"&gt;test suite&lt;/a&gt; that assesses Keycloak performance. After careful study, I decided not to use it for two reasons:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;It does not answer the questions I listed in the previous section.&lt;/li&gt; &lt;li&gt;The test suite is tightly coupled with the Keycloak development environment, so it would be difficult to reuse on customer sites if needed.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;I divided my approach into four main steps:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Setting up SSO and the underlying services (database, load balancer, etc.).&lt;/li&gt; &lt;li&gt;Filling the SSO database with users, clients, and realms.&lt;/li&gt; &lt;li&gt;Generating load on the SSO server.&lt;/li&gt; &lt;li&gt;Collecting performance data.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;Every customer is different, and sometimes various tools and techniques might be required on customer sites. With that in mind, I designed those four steps to be loosely coupled, so you can adjust each step to use a different tool or technique.&lt;/p&gt; &lt;p&gt;Whenever possible, I reuse the &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_single_sign-on/7.4/html/server_administration_guide/export_import"&gt;Keycloak Realm file&lt;/a&gt; as a pivot format. It is used by the script that loads the database and by the load testing tool that generates load on the SSO server.&lt;/p&gt; &lt;p&gt;To set up SSO and the underlying services, I chose to use &lt;a href="https://github.com/nmasse-itix/keycloak-loadtesting-setup/tree/33168b763c3f27486d0e883b7b008783835f3b22"&gt;Ansible playbooks&lt;/a&gt; that deploy components as Podman containers. They are easy for new team members to use and understand; plus, they are widely used on customer sites.&lt;/p&gt; &lt;p&gt;I created a dedicated tool named &lt;a href="https://github.com/nmasse-itix/keycloak-import-realm/tree/v0.1.4"&gt;kci&lt;/a&gt; to load the database with users, clients, and realms.&lt;/p&gt; &lt;p&gt;To generate the load on the SSO server, I used &lt;a href="https://github.com/nmasse-itix/keycloak-loadtesting-k6/tree/2475c668116b2591fd9c88fb6caa5c9d77c66e18"&gt;K6&lt;/a&gt;, a novel performance testing tool written in Go that uses plain JavaScript for the test definition. (Have a look at &lt;a href="https://k6.io/"&gt;k6.io&lt;/a&gt; if you aren't familiar with it.)&lt;/p&gt; &lt;p&gt;The test results are collected by Prometheus and presented through Grafana, as shown in Figure 1. For a primer on K6, Prometheus, and Grafana, I recommend reading &lt;a href="https://www.itix.fr/blog/how-to-run-performance-tests-with-k6-prometheus-grafana/"&gt;this article.&lt;/a&gt;&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/blog/2021/04/k6-grafana-prometheus.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/blog/2021/04/k6-grafana-prometheus.png?itok=sGFLqEvy" width="600" height="321" alt="K6, Grafana and Prometheus conducting a benchmark on Red Hat SSO." title="k6, grafana &amp;amp; prometheus" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; K6, Grafana and Prometheus conducting a benchmark on Red Hat SSO. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 1: K6, Grafana, and Prometheus conducting a benchmark on SSO.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Designing the benchmark scenarios&lt;/h2&gt; &lt;p&gt;Scenarios are a key part of the performance benchmark. Carefully chosen scenarios will help to answer the questions we established earlier. One should devise scenarios as scientific experiments:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Choose a control that sets a baseline and ensures that performance remains constant at any point in time.&lt;/li&gt; &lt;li&gt;Craft experiments by changing one (and only one) parameter of the control experiment at a time. The experiment will reflect this parameter's effect on performance.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;For my control experiment, I chose the following configuration as the baseline:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Two SSO servers, each one having a dedicated single physical core.&lt;/li&gt; &lt;li&gt;The SSO servers are backed by a PostgreSQL instance.&lt;/li&gt; &lt;li&gt;A Traefik reverse proxy is set in front of the SSO servers to spread the load.&lt;/li&gt; &lt;li&gt;The database is loaded with 5,000 users and 500 clients spread amongst 5 realms.&lt;/li&gt; &lt;li&gt;No specific performance tuning is applied to any of those components.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;And then from this baseline, I devised the following scenarios:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;strong&gt;Offline tokens:&lt;/strong&gt; Same as baseline, but offline tokens are requested instead of regular tokens.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;MariaDB:&lt;/strong&gt; Same as baseline, but with MariaDB instead of PostgreSQL.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;One node:&lt;/strong&gt; Same as baseline, but with only one SSO instance having two physical cores.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Size S:&lt;/strong&gt; Same as baseline, but with less data in the database (100 users and 10 clients in 1 realm).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Size L:&lt;/strong&gt; Same as baseline, but with more data in the database (100,000 users and 10,000 clients spread in 10 realms).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;PBKDF2 with 1 iteration:&lt;/strong&gt; Same as baseline, but with the PBKDF2 configured with 1 iteration instead of 27,500.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LDAP:&lt;/strong&gt; Same as baseline, but with users loaded in an OpenLDAP instance instead of the SSO database.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;In the baseline and the scenarios just described, I chose to collect the following metrics:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;The user opens its SSO session: How many TPS?&lt;/li&gt; &lt;li&gt;The user refreshes its access token: How many TPS?&lt;/li&gt; &lt;li&gt;The user token is introspected using the tokeninfo endpoint: How many TPS?&lt;/li&gt; &lt;li&gt;The user token is introspected using the userinfo endpoint: How many TPS?&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;I chose to focus only on the number of transactions per second because it is an objective measure (the maximum you can get). Latency figures are sometimes discussed on customer sites, but there is not much we can do about it. Latency refers to the minimum number of CPU cycles required to serve the request, and it can only increase when bottlenecks (such as CPU contention) start to appear. Said differently: There is a typical latency, and that latency starts to skyrocket when a tipping point is passed. As long as you do not cross that point, there is nothing interesting happening.&lt;/p&gt; &lt;p&gt;I ran the performance benchmarks on a bare-metal server: an HP MicroServer gen8, with a &lt;a href="https://www.cpubenchmark.net/cpu.php?cpu=Intel+Xeon+E3-1240+V2+%40+3.40GHz&amp;id=1190"&gt;Xeon E3-1240v2 CPU&lt;/a&gt; and 16GB of RAM. Only two physical cores of the Xeon CPU are dedicated to SSO servers. The rest has been allocated to the load balancer, the database, and the operating system.&lt;/p&gt; &lt;h2&gt;Note on the PBKDF2 function&lt;/h2&gt; &lt;p&gt;In the next section, you will see a big increase in the throughput depending on where the user passwords are stored. Let's take a closer look at the Password-Based Key Derivation Function 2 (PBKDF2) function.&lt;/p&gt; &lt;p&gt;By default, Red Hat's single sign-on tool stores the user passwords in its internal database and hashes those passwords using the PBKDF2 function. The purpose of this function is to be CPU intensive to slow down brute force attacks to a point where they become too expensive or too long to be practical. One can adjust the strength of this protection by configuring the number of iterations.&lt;/p&gt; &lt;p&gt;SSO performs 27,500 PBKDF2 iterations by default. &lt;a href="https://en.wikipedia.org/wiki/PBKDF2"&gt;Wikipedia&lt;/a&gt; tells us more about what is a safe choice for the number of iterations.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: When the standard was written in the year 2000, the recommended minimum number of iterations was 1,000, but the parameter was designed to increase over time to align with CPU speeds. A Kerberos standard in 2005 recommended 4,096 iterations; Apple reportedly used 2,000 for iOS 3, and 10,000 for iOS 4. In 2011, LastPass used 5,000 iterations for JavaScript clients and 100,000 iterations for server-side hashing.&lt;/p&gt; &lt;p&gt;This means you cannot store passwords in a secure way and at the same time exhibit a high number of TPS per physical core during the user session openings. By definition.&lt;/p&gt; &lt;p&gt;However, you can configure SSO to use passwords from another repository (your Active Directory, OpenLDAP, or Red Hat Directory Server, for instance) and rely on the security mechanisms of those repositories. That would be a way to have the best of both worlds.&lt;/p&gt; &lt;h2&gt;Results&lt;/h2&gt; &lt;p&gt;Based on the results, I was able to draw the following conclusions:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;The key dimensions of an SSO project are the number of user session openings per second and where the user passwords are stored. &lt;ul&gt;&lt;li&gt;SSO can sustain around 75 TPS per physical core if the user passwords are stored in a third-party system (an LDAP directory, for instance) or if the PBKDF2 function is configured with one iteration.&lt;/li&gt; &lt;li&gt;Otherwise, SSO sustains slightly less than 10 TPS per physical core.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Refreshing tokens is less costly: SSO can sustain around 200 TPS per physical core.&lt;/li&gt; &lt;li&gt;Introspecting a token is pretty cheap. SSO can sustain around 1,400–1,700 TPS per physical core; 1,400 TPS using the tokeninfo endpoint, and 1,700 TPS using the userinfo endpoint.&lt;/li&gt; &lt;li&gt;The choice of the database has no significant impact on performance.&lt;/li&gt; &lt;li&gt;Using offline tokens instead of regular tokens has a slight impact on performance (a 10% penalty).&lt;/li&gt; &lt;li&gt;When high availability is not required, the one node setup shows a 20% increase in the number of TPS per physical core.&lt;/li&gt; &lt;/ul&gt;&lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: One physical core = two threads = two vCPU.&lt;/p&gt; &lt;p&gt;See &lt;a href="https://github.com/nmasse-itix/keycloak-loadtesting/tree/35d4284be5a03a089c21f7b7baa6611b04edf7f7"&gt;the repository containing the complete result set.&lt;/a&gt;&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;The SSO performance benchmarks presented in this article are by no means a definitive answer to this topic. It should instead be considered as an initial work to help the community, Red Hatters, and our customers better size their single sign-on projects.&lt;/p&gt; &lt;p&gt;More work is required to test other hypotheses, such as the impact of an external &lt;a href="https://developers.redhat.com/products/datagrid/overview"&gt;Red Hat Data Grid&lt;/a&gt; server, possible optimizations here and there, the possibility of achieving linear scalability with a high number of nodes, or even the impact of deployments within &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/06/07/how-size-your-projects-red-hats-single-sign-technology" title="How to size your projects for Red Hat's single sign-on technology"&gt;How to size your projects for Red Hat's single sign-on technology&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/8lHuPmBeSk4" height="1" width="1" alt=""/&gt;</summary><dc:creator>Nicolas Massé</dc:creator><dc:date>2021-06-07T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/06/07/how-size-your-projects-red-hats-single-sign-technology</feedburner:origLink></entry><entry><title type="html">Retail data framework - Example data architecture</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/rLdS-0pw4Qw/retail-data-framework-example-data-architecture.html" /><author><name>Eric D. Schabell</name></author><id>http://feedproxy.google.com/~r/schabell/jboss/~3/0kGi-3IJtTg/retail-data-framework-example-data-architecture.html</id><updated>2021-06-07T05:00:00Z</updated><content type="html">Part 3 - Example data architecture In our  from this series shared a look at the logical common architectural elements found in a retail data framework solution for retail organisations. The process was laid out how we've approached the use case and how portfolio solutions are the base for researching a generic architectural blueprint.  It continued by laying out the process of how we've approached the use case by researching successful customer portfolio solutions as the basis for a generic architectural blueprint. Having completed our discussions on the logical view of the blueprint, it's now time to look at a specific example. This article walks you through an example stock control scenario showing how expanding the previously discussed elements provides a blueprint for your own stock control scenarios. BLUEPRINTS REVIEW As mentioned before, the architectural details covered here are base on real solutions using open source technologies. The example scenario presented here is a generic common blueprint that was uncovered researching those solutions. It's our intent to provide a blueprint that provides guidance and not deep technical details. This section covers the visual representations as presented, but it's expected that they'll be evolving based on future research. There are many ways to represent each element in this architectural blueprint, but we've chosen a format that we hope makes it easy to absorb. Feel free to post comments at the bottom of this post, or  with your feedback. Now let's take a look at the details in this blueprint and outline the solution for a retail data framework architecture solution. RETAIL DATA FRAMEWORK This data framework presents an organisational wide view of how data is being managed and is lined to from many of the other retail architecture blueprints we've published. Those use cases are often feeding data into this architecture, and a few are depending on data provided back from this framework. Our first walk though this architecture is showing network connections only and will be followed by a data flow view in the next section. Starting on the left side, we find all the actors such as shoppers, colleagues, and associates. Another user is the IoT devices that these actors are leveraging on the edge of the network to connect to the data framework. Finally, there are links here to the other actors that represent users of the data framework, such as point of sale analytics, customer analytics, market analysis, and central IT. This last group of actors is very interested in leveraging the data framework to gain insights into their specific retail organisational domains. In the center of the diagram you find the data integration platform, a collection of services and applications owned and created by the retail organisation development teams. As the actors are interacting with all these services through the various web application. Before connecting with eventual backing services, the API management services ensure that only authorised and authenticated calls can reach those backing services.  With most of the applications needs being that of representing collected data, it's natural to find data visualisation microservices and data caching microservices backing them. Both of these microservice collections make use of the more generic integration data microservices to interact with the available storage services. These storage services can be any of the following depending on the retail organisations choices with regards to storage; data lakes, data warehouse, data hub, and data marts. A very important aspect to these types of architectures is that of combining both messaging and event processing capabilities in a complimentary fashion. While events might trigger business automation processes and/or the need for compliancy and regulatory rules checks, they might be proceeded by message transformation before they can be consumed by the end service. In the process automation you'll often find the need to use integration microservices for access to the core platform systems and data science platform services. Going to the bottom core platform you'll find internal tooling that can be from legacy systems or just a compilation of systems and tools used to cover one of the four categories of functionality shown here. First, there is the compliance and regulatory tooling, followed by governance tooling, then auditing tooling, and finally authentication and authorisation systems.  On the far right, the data science platform can contain any of the following systems or featured technologies to help discover the hidden gems in all the available data that the retail organisation has collected. There can be business intelligence tooling, or data visualisation tooling, and of course the core business of the platform has a data science (AI / ML) element.  All of these elements and platforms make up the retail data framework physical architecture. Next we'll take a nuanced look at the data flow in our architecture blueprint.  RETAIL DATA FRAMEWORK (DATA) This look at a retail data framework architecture blueprint data flow is not meant to be an all encompassing view of the exact flow. The idea is to give a blueprint that you use to understand how an actor and their data works through the entire retail data framework. With that in mind, the data flow shown is from the actors on the right and works its way through the data integration platform and uses the storage services, core platform, and data science platform to provide the data views the actors are looking for. Most of the arrows indicate a data flow from the actors to the backend systems, though it's not hard to imagine the response data flows working back to the actors. This is left as an exercise for the reader. This concludes the look at the most pervasive of the use cases we've discussed on our tours of the retail architecture blueprints. The retail data framework is a foundational need that most organisations spend a lot of time and investment to leverage.  WHAT'S NEXT This was just a short overview of the example data architecture that makes up our architecture blueprint for the retail data framework use case.  An overview of this series on retail data framework portfolio architecture blueprint: 1. 2. 3. Catch up on any past articles you missed by following any published links above. This completes the series and we hope you enjoyed this architecture blueprint for retail data framework in retail. (Article co-authored by , Chief Architect Retail, Red Hat)&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/rLdS-0pw4Qw" height="1" width="1" alt=""/&gt;</content><dc:creator>Eric D. Schabell</dc:creator><feedburner:origLink>http://feedproxy.google.com/~r/schabell/jboss/~3/0kGi-3IJtTg/retail-data-framework-example-data-architecture.html</feedburner:origLink></entry><entry><title>An easier way to go: SCTP over UDP in the Linux kernel</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/_uKufECPE_I/easier-way-go-sctp-over-udp-linux-kernel" /><author><name>Long Xin</name></author><id>1a8d198b-2430-4971-843a-cca24741e088</id><updated>2021-06-04T07:00:00Z</updated><published>2021-06-04T07:00:00Z</published><summary type="html">&lt;p&gt;Stream Control Transmission Protocol over User Datagram Protocol (SCTP over UDP, also known as UDP encapsulation of SCTP) is a feature defined in &lt;a href="https://datatracker.ietf.org/doc/html/rfc6951"&gt;RFC6951&lt;/a&gt; and implemented in the &lt;a href="https://developers.redhat.com/topics/linux"&gt;Linux&lt;/a&gt; kernel space since 5.11.0. It is planned to be supported by &lt;a href="https://developers.redhat.com/products/rhel/overview"&gt;Red Hat Enterprise Linux&lt;/a&gt; (RHEL) 8.5.0 and 9.0.&lt;/p&gt; &lt;p&gt;This article is a quick introduction to SCTP over UDP in the Linux kernel.&lt;/p&gt; &lt;h2&gt;Why we need SCTP over UDP&lt;/h2&gt; &lt;p&gt;As described in the Internet Engineering Task Force Request for Comments (RFC), there are two main reasons that we need SCTP over UDP:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;To allow SCTP traffic to pass through legacy NATs, which do not provide native SCTP support as specified in [BEHAVE] and [NATSUPP]. To allow SCTP to be implemented on hosts that do not provide direct access to the IP layer. In particular, applications can use their own SCTP implementation if the operating system does not provide one.&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The first reason will solve the middlebox issues that have brought many troubles to users and prevented SCTP’s wide use. The second reason is to allow user space applications to develop their own SCTP implementation based on the UDP protocol.&lt;/p&gt; &lt;h2&gt;How SCTP over UDP works&lt;/h2&gt; &lt;p&gt;With this feature enabled, all SCTP packets are encapsulated into UDP packets. SCTP over UDP is implemented with kernel UDP tunnel APIs that have previously been used by the VXLAN, GENEVE, and TIPC protocols.&lt;/p&gt; &lt;p&gt;For receiving encapsulated packets, the kernel listens on one specific UDP port on all local interfaces. The default port is 9899. This port also acts as the UDP packet &lt;code&gt;src&lt;/code&gt; port for this host as a sender. As you might anticipate, the &lt;code&gt;dest&lt;/code&gt; port should be the listening port of the peer, which also defaults to 9899. The &lt;code&gt;src&lt;/code&gt; and &lt;code&gt;dest&lt;/code&gt; addresses bound to by SCTP would still be used in IP headers:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt; +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | IP(v6) Header (addresses bound by SCTP) | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | UDP Header (src: 9899, dest: 9899) | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | SCTP Common Header (SCTP ports) | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | SCTP Chunk #1 | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | ... | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | SCTP Chunk #n | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ &lt;/code&gt;&lt;/pre&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: SCTP also considers the UDP header while calculating the fragmentation point.&lt;/p&gt; &lt;h2&gt;How to use SCTP over UDP&lt;/h2&gt; &lt;p&gt;When programming, you don't need to do anything different: All the standard SCTP features still apply, and all the APIs are available to use as before. Old applications will work well without any changes or recompilation. The only adjustment is to set up a UDP port (a local listening port or &lt;code&gt;src&lt;/code&gt; port) and an &lt;em&gt;encapsulation&lt;/em&gt; port (a remote listening or &lt;code&gt;dest&lt;/code&gt; port), which could be done globally for the network namespace by &lt;code&gt;sysctl&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt; # sysctl -w net.sctp.encap_port=9899 # sysctl -w net.sctp.udp_port=9899&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Alternatively, you could set the encapsulation port per socket, association, or transport, using &lt;code&gt;sockopt&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt; setsockopt(SCTP_REMOTE_UDP_ENCAPS_PORT, port);&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;On the server side, the &lt;code&gt;encapsulation&lt;/code&gt; port normally doesn’t need to be set explicitly, as detailed in the next section.&lt;/p&gt; &lt;h2&gt;The UDP encapsulation port&lt;/h2&gt; &lt;p&gt;The UDP encapsulation port allows for very flexible usage. On the sender side, the global encapsulation port only provides a default value:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;The &lt;em&gt;per-socket&lt;/em&gt; encapsulation port can be used when another socket on one host connects to a different host on which a different UDP port is used.&lt;/li&gt; &lt;li&gt;The &lt;em&gt;per-association&lt;/em&gt; encapsulation port can be used when the same socket connects to a different host on which a different UDP port is used.&lt;/li&gt; &lt;li&gt;The &lt;em&gt;per-transport&lt;/em&gt; encapsulation port can be used when the same association wants to send UDP-encapsulated SCTP packets on one transport.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;On the receiver side, the encapsulation port normally doesn’t need to be set:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;The encapsulation port of one association would be learned from the first &lt;code&gt;INIT&lt;/code&gt; packet. Other &lt;code&gt;INIT&lt;/code&gt;s with different UDP &lt;code&gt;src&lt;/code&gt; ports would then be discarded.&lt;/li&gt; &lt;li&gt;The encapsulation port of each transport would be learned from the incoming packets on the corresponding path, and can be updated anytime.&lt;/li&gt; &lt;li&gt;Plain SCTP packets can still be processed even if the encapsulation ports of the association and its transports are set.&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;If you’re using SCTP and enjoying its features, like multi-homing, multi-streaming, and partial-reliability, but having issues with middleboxes, the Linux kernel now provides an easier way to get around them.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/06/04/easier-way-go-sctp-over-udp-linux-kernel" title="An easier way to go: SCTP over UDP in the Linux kernel"&gt;An easier way to go: SCTP over UDP in the Linux kernel&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/_uKufECPE_I" height="1" width="1" alt=""/&gt;</summary><dc:creator>Long Xin</dc:creator><dc:date>2021-06-04T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/06/04/easier-way-go-sctp-over-udp-linux-kernel</feedburner:origLink></entry><entry><title type="html">WildFly 24 Beta1 is released!</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/_4VPIIaiF9M/" /><author><name>Brian Stansberry</name></author><id>https://wildfly.org//news/2021/06/04/WildFly24-Beta-Released/</id><updated>2021-06-04T00:00:00Z</updated><content type="html">I’m pleased to announce that the new WildFly and WildFly Preview 24.0.0.Beta1 releases are available for download at . Work during the WildFly 24 development cycle has been primarily oriented toward bug fixing, plus the . But I do want to express my special thanks to Sonia Zaldana, a great contributor to WildFly over the past year, who has added three new features in WildFly 24 Beta1: * * * The other area of focus during this development cycle was improving how WildFly, particularly WildFly Preview, runs on JDK 16 and the early access releases of the next LTS JDK release, JDK 17. There are still some issues to resolve, but WildFly Preview 24 Beta1 runs well enough on the latest JDKs that it’s worthwhile for people interested in what JDK 17 will mean for their application to give it a look. The release notes for the release are , with issues fixed in the underlying WildFly Core betas listed . Please try it out and give us your feedback, while we get to work WildFly 24 Final! Best regards, Brian&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/_4VPIIaiF9M" height="1" width="1" alt=""/&gt;</content><dc:creator>Brian Stansberry</dc:creator><feedburner:origLink>https://wildfly.org//news/2021/06/04/WildFly24-Beta-Released/</feedburner:origLink></entry><entry><title>DevNation Deep Dive: Argo CD</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/NWxqEzgkyEY/devnation-deep-dive-argo-cd" /><author><name>Natale Vinto</name></author><id>3aa32954-3e4f-4594-9dcf-227e5e9a78a7</id><updated>2021-06-03T15:24:28Z</updated><published>2021-06-03T15:24:28Z</published><summary type="html">&lt;p&gt;Argo CD is a declarative &lt;a href="https://developers.redhat.com/topics/ci-cd/"&gt;continuous delivery&lt;/a&gt; tool for &lt;a href="https://developers.redhat.com/topics/kubernetes/"&gt;Kubernetes&lt;/a&gt;. It follows the &lt;a href="https://developers.redhat.com/topics/gitops"&gt;GitOps&lt;/a&gt; pattern of using Git repositories as the source of truth for defining the desired application state.&lt;/p&gt; &lt;p&gt;Argo CD automates the deployment of desired application states in specified target environments. Application deployments can track updates to branches, tags, or pin to a specific version of manifests at a Git commit.&lt;/p&gt; &lt;h2&gt;Take a deep dive into Argo CD and GitOps&lt;/h2&gt; &lt;p&gt;Learn the basics of Argo CD and GitOps in this &lt;a href="https://developers.redhat.com/devnation/deep-dive"&gt;DevNation Deep Dive&lt;/a&gt;. Courses are offered in five languages (English, Italian, Spanish, French, and Brazilian Portuguese) across multiple time zones.&lt;/p&gt; &lt;p&gt;The Argo CD Deep Dive will cover:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;The fundamentals of GitOps and Argo CD&lt;/li&gt; &lt;li&gt;How to install and manage Argo CD in Kubernetes and &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Deploying and syncing applications&lt;/li&gt; &lt;li&gt;Using Kustomize with Argo CD&lt;/li&gt; &lt;li&gt;Using sync waves and hooks with Argo CD&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Sessions begin June 8. &lt;a href="https://developers.redhat.com/devnation/deep-dive/argocd"&gt;Register now!&lt;/a&gt;&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/06/03/devnation-deep-dive-argo-cd" title="DevNation Deep Dive: Argo CD"&gt;DevNation Deep Dive: Argo CD&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/NWxqEzgkyEY" height="1" width="1" alt=""/&gt;</summary><dc:creator>Natale Vinto</dc:creator><dc:date>2021-06-03T15:24:28Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/06/03/devnation-deep-dive-argo-cd</feedburner:origLink></entry><entry><title type="html">New DMN Boxed Expressions Editor</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/X-6gRthc7Ec/new-dmn-boxed-expression-editor.html" /><author><name>Valentino Pellegrino</name></author><id>https://blog.kie.org/2021/06/new-dmn-boxed-expression-editor.html</id><updated>2021-06-03T07:39:00Z</updated><content type="html">While working with the DMN editor, you may have noticed that a big amount of modeling logic lives on the expressions, boxed in a few kinds of DMN nodes.  The Boxed Expressions are crucial for modeling decision logic. On that side, there is an ongoing effort from the engineering team to improve the user experience of the Boxed Expression Editor, which will be described in a series of articles that will clarify reasons, solutions, and the next steps. This first article will briefly explore Boxed Expressions from their perspective, looking at how they are currently leveraged in the DMN editor. WHAT ARE BOXED EXPRESSIONS A Decision Requirements Diagram (DRD) is a graphical representation of the decision model structure. In such a diagram, you can define decision nodes and each of them has a value expression (or decision logic) that determines its output based on its current inputs. The logic defined in such expressions is expressed in tabular formats called boxed expressions. The DMN editor supports a variety of decision logic types: * Decision Table: it is the most familiar type of boxed expression. Its logic is based on rules. It is basically a table where the columns on the left represent the inputs, and the column on the right, instead, is the output. Each row is a decision rule. If all cells belonging to the input columns of a decision rule match the input data, then the output of the decision rule gets selected. On the top-left corner of the table, there is a hit policy, which specifies what the result of the Decision Table is in cases of overlapping rules. Below, a possible layout for the Decision Table, taken directly from the spec. * Literal Expression: a single cell that represents a formula, using the expression language. Below, an example of Literal Expression. * Invocation: it’s an expression that maps inputs to parameterized decision logic. In the example below, the invoked decision logic is a business knowledge model. * List: a collection of items. When cells are arranged vertically, the layout is similar to: * Context: a collection of entries (name, value) with an optional result value. It can have nested values. The context when it is in a vertical layout looks like the following: * Relation: it’s a table representing only values. Quite simple. A collection of columns and rows represent a Relation: * Function: it’s a notation for parameterized boxed expressions. It has three cells: the function kind, the list of parameters, and the function body. Below you can find a Function layout: BOXED EXPRESSIONS IN THE CURRENT DMN EDITOR I am sure you have already used the DMN Editor to edit a Boxed Expression. By the way, let’s take one step at a time. First of all, let’s go to the well-known website. Here, there is a ready-to-use online version of the DMN editor: You have two ways for editing a Boxed Expression: 1. Edit a Business Knowledge Model node. In this case, you are able to edit only its Function definition. 2. Edit a Decision Service node. In this way, it is possible to specify the desired expression logic type by using a selector. Editor look &amp;amp; feel is very close to what the spec says.  Let’s take, as an example, the Literal Expression, which is the most minimal logic type.  You can navigate back to the DMN Diagram by clicking the link on the top. The section below contains both the decision node name and the selected logic type. On the bottom, we have the boxed expression itself, with the header containing the value expression’s name and type ref, and a body containing the text field, that you can fill with FEEL code. I suggest you play with the editor, trying to select other logic types. For example, what happens if you need to represent a Context table, where one entry maps to a Literal Expression, and another maps to a List of Literal Expressions? Well, you can find out that there are several business cases where it is necessary to specify nested expressions and we have few logic types that suit such cases well. CONCLUSION We have covered just a few topics related to Boxed Expressions. The main goal here was to communicate how crucial they are for the DMN ecosystem. You will find more information about DMN modeling and related use-cases on the official . The existing Boxed Expressions editor already looks great, so what may you expect from the new one? The answer will be part of the next article! We will talk about the state of the art for this editor, exploring rooms for enhancements, and how we’re improving the overall user experience in the new one. The post appeared first on .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/X-6gRthc7Ec" height="1" width="1" alt=""/&gt;</content><dc:creator>Valentino Pellegrino</dc:creator><feedburner:origLink>https://blog.kie.org/2021/06/new-dmn-boxed-expression-editor.html</feedburner:origLink></entry><entry><title>Red Hat Software Collections 3.7 and Red Hat Developer Toolset 10.1 now generally available</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/_mxUp6SVqAE/red-hat-software-collections-37-and-red-hat-developer-toolset-101-now-generally" /><author><name>Brian Gollaher</name></author><id>3ff7100c-f6c1-4a6f-b8de-03b2443f0f71</id><updated>2021-06-03T07:00:00Z</updated><published>2021-06-03T07:00:00Z</published><summary type="html">&lt;p&gt;The latest versions of &lt;a href="https://developers.redhat.com/products/softwarecollections/overview"&gt;Red Hat Software Collections&lt;/a&gt; and &lt;a href=" /products/developertoolset/overview"&gt;Red Hat Developer Toolset&lt;/a&gt; are now generally available. Red Hat Software Collections 3.7 delivers the latest stable versions of many popular open source runtime languages, web servers, and databases natively to the &lt;a href="https://developers.redhat.com/products/rhel/overview"&gt;world’s leading enterprise Linux platform&lt;/a&gt;. These components are supported for up to five years for a more consistent, efficient, and reliable developer experience.&lt;/p&gt; &lt;h2&gt;What's new in Red Hat Software Collections 3.7&lt;/h2&gt; &lt;p&gt;New and updated collections in the latest release of Red Hat Software Collections include:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;strong&gt;PostgreSQL 13&lt;/strong&gt;: This version provides a number of new features and enhancements over version 12. Notable changes include performance improvements resulting from de-duplication of B-tree index entries, improved performance for queries that use aggregates or partitioned tables, improved query planning when using extended statistics, parallelized vacuuming of indexes, and incremental sorting.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;MariaDB 10.5&lt;/strong&gt;: Notable enhancements over the previously available version 10.3 include a number of security updates, new features, and updates to the InnoDB storage engine. In addition, the MariaDB Galera Cluster has been upgraded to version 4.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ruby 3.0&lt;/strong&gt;: This version provides a number of bug fixes and enhancements over the previously released Ruby 2.7. Being a new major version, it brings speed improvements, introduces language to describe the types (RBS), and provides concurrent abstraction via Ractor. The new version now also separates keyword arguments from other arguments.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Java Mission Control 8.0.0 (update)&lt;/strong&gt;: This is an advanced set of tools for managing, monitoring, profiling, and troubleshooting Java applications. Updates to Java Mission Control include adding new graphs and viewers for stack traces, threads, and memory usage.&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Developer Toolset 10.1&lt;/h2&gt; &lt;p&gt;Also new in Red Hat Software Collections 3.7 is Developer Toolset 10.1, which features GNU Compiler Collection (GCC) 10, an updated, curated collection of compilers, toolchains, debuggers, and other critical development tools. Forming the foundation of Developer Toolset 10 is GCC 10.2.1, a new update of the popular open source compiler collection. Additional updates in Developer Toolset 10.1 center on delivering new updates of &lt;a href="https://developers.redhat.com/topics/c"&gt;C/C++&lt;/a&gt; and Fortran debugging and &lt;a href="https://developers.redhat.com/blog/category/performance/"&gt;performance&lt;/a&gt; tools.&lt;/p&gt; &lt;p&gt;All new collections in Red Hat Software Collections 3.7 are also available as &lt;a href="https://connect.redhat.com/explore/red-hat-container-certification"&gt;Red Hat Certified Containers&lt;/a&gt; through the &lt;a href="https://catalog.redhat.com/software/containers/explore"&gt; Red Hat Ecosystem Catalog&lt;/a&gt;. This makes it easier to build and deploy applications using the supported components of Red Hat Software Collections for &lt;a href="https://developers.redhat.com/products/rhel/overview"&gt;Red Hat Enterprise Linux&lt;/a&gt; and &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt; environments.&lt;/p&gt; &lt;p&gt;Red Hat Software Collections 3.7 continues Red Hat’s commitment to customer choice in terms of the underlying compute architecture, with availability across x86_64, ppc64, ppc64le, and s390x hardware.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;Red Hat customers with active &lt;a href="https://developers.redhat.com/blog/2019/08/21/why-you-should-be-developing-on-red-hat-enterprise-linux/"&gt;Red Hat Enterprise Linux&lt;/a&gt; subscriptions can access Red Hat Software Collections via the &lt;a href="https://access.redhat.com/solutions/472793"&gt;Red Hat Software Collections repository&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;For more information, please read the full &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_software_collections/3/"&gt;release notes&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/06/03/red-hat-software-collections-37-and-red-hat-developer-toolset-101-now-generally" title="Red Hat Software Collections 3.7 and Red Hat Developer Toolset 10.1 now generally available"&gt;Red Hat Software Collections 3.7 and Red Hat Developer Toolset 10.1 now generally available&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/_mxUp6SVqAE" height="1" width="1" alt=""/&gt;</summary><dc:creator>Brian Gollaher</dc:creator><dc:date>2021-06-03T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/06/03/red-hat-software-collections-37-and-red-hat-developer-toolset-101-now-generally</feedburner:origLink></entry><entry><title>Simulating CloudEvents with AsyncAPI and Microcks</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/CKaov6paM0c/simulating-cloudevents-asyncapi-and-microcks" /><author><name>Laurent Broudoux</name></author><id>6b56bdb0-3134-4971-9897-3d172f4ea6ad</id><updated>2021-06-02T07:00:00Z</updated><published>2021-06-02T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://developers.redhat.com/topics/event-driven"&gt;Event-driven architecture&lt;/a&gt; was an evolutionary step toward cloud-native applications, and supports &lt;a href="https://developers.redhat.com/topics/serverless-architecture"&gt;serverless&lt;/a&gt; applications. Events connect &lt;a href="https://developers.redhat.com/topics/microservices"&gt;microservices&lt;/a&gt;, letting you decouple functions in space and time and make your applications more resilient and elastic.&lt;/p&gt; &lt;p&gt;But events come with challenges. One of the first challenges for a development team is how to describe events in a repeatable, structured form. Another challenge is how to work on applications that consume events without having to wait for another team to hand you the applications that produce those events.&lt;/p&gt; &lt;p&gt;This article explores those two challenges and shows how to simulate events using &lt;a href="https://cloudevents.io"&gt;CloudEvents&lt;/a&gt;, &lt;a href="https://asyncapi.com"&gt;AsyncAPI&lt;/a&gt;, and &lt;a href="https://microcks.io"&gt;Microcks&lt;/a&gt;. CloudEvents and AsyncAPI are complementary specifications that you can combine to help define an event-driven architecture. Microcks allows simulation of CloudEvents to speed up and protect the autonomy of development teams.&lt;/p&gt; &lt;h2&gt;CloudEvents or AsyncAPI?&lt;/h2&gt; &lt;p&gt;New standards such as &lt;a href="https://cloudevents.io/"&gt;CloudEvents&lt;/a&gt; or &lt;a href="https://www.asyncapi.com/"&gt;AsyncAPI&lt;/a&gt; have emerged to address the need to describe events in a structured format. People often ask: "Should I use CloudEvents or AsyncAPI?" There's a widespread belief that CloudEvents and AsyncAPI compete within the same scope. I see things differently, and in this article, I'll explain how the two standards work well together.&lt;/p&gt; &lt;h3&gt;What is CloudEvents?&lt;/h3&gt; &lt;p&gt;The essence of CloudEvents can be found on a statement from &lt;a href="http://cloudevents.io/"&gt;its website&lt;/a&gt;:&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;em&gt;CloudEvents is a specification for describing event data in common formats to provide interoperability across services, platforms, and systems.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;The purpose of CloudEvents is to establish a common format for event data description. CloudEvents is part of the Cloud Native Computing Foundation's &lt;a href="https://github.com/cncf/wg-serverless"&gt; Serverless Working Group&lt;/a&gt;. A lot of integrations already exist within &lt;a href="https://knative.dev/docs/eventing/"&gt;Knative Eventing&lt;/a&gt; (or &lt;a href="https://developers.redhat.com/topics/serverless-architecture"&gt;Red Hat OpenShift Serverless&lt;/a&gt;), &lt;a href="https://www.triggermesh.com/"&gt;Trigger Mesh&lt;/a&gt;, and &lt;a href="https://azure.microsoft.com/en-us/services/event-grid"&gt;Azure Event Grid&lt;/a&gt;, allowing true cross-vendor platform interoperability.&lt;/p&gt; &lt;p&gt;The CloudEvents specification is focused on events and defines a common envelope (set of attributes) for your application event. As of today, CloudEvents proposes two different &lt;a href="https://github.com/cloudevents/spec/blob/v1.0.1/kafka-protocol-binding.md#13-content-modes"&gt;content modes&lt;/a&gt; for transferring events: structured and binary.&lt;/p&gt; &lt;p&gt;The CloudEvents repository offers an &lt;a href="https://gist.github.com/e3261b13eb7a9dbb14ccf59b1580d5b7#file-cloudevent-json"&gt;example of a JSON structure containing event attributes&lt;/a&gt;. This is a &lt;em&gt;structured CloudEvent&lt;/em&gt;. The event data in the example is XML, contained in the value &lt;code&gt;&lt;much wow=\"xml\"/&gt;&lt;/code&gt;, but it can be of any type. CloudEvents takes care of defining meta-information about your event, but does not help you define the actual event content:&lt;/p&gt; &lt;pre&gt; { "specversion" : "1.0.1", "type" : "com.github.pull.create", "source" : "https://github.com/cloudevents/spec/pull/123", "id" : "A234-1234-1234", "time" : "2020-04-05T17:31:00Z", "comexampleextension1" : "value", "comexampleextension2" : { "othervalue": 5 }, "contenttype" : "text/xml", "data" : "&lt;much wow=\"xml\"/&gt;" } &lt;/pre&gt; &lt;h3&gt;What is AsyncAPI?&lt;/h3&gt; &lt;p&gt;To understand AsyncAPI, again we turn to &lt;a href="http://asyncapi.com/"&gt;its website&lt;/a&gt;:&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;em&gt;AsyncAPI is an industry standard for defining asynchronous APIs. Our long-term goal is to make working with EDAs as easy as it is to work with REST APIs.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;The "API" stands for &lt;a href="https://developers.redhat.com/topics/api-management"&gt;application programming interface&lt;/a&gt; and embodies an application's interactions and capabilities. AsyncAPI can be seen as the sister specification of &lt;a href="https://www.openapis.org/"&gt;OpenAPI&lt;/a&gt;, but targeting asynchronous protocols based on event brokering.&lt;/p&gt; &lt;p&gt;AsyncAPI focuses on the application and the communication channels it uses. Unlike CloudEvents, AsyncAPI does not define how your events should be structured. However, AsyncAPI provides an extended means to precisely define both the meta-information and the actual content of an event.&lt;/p&gt; &lt;p&gt;An &lt;a href="https://gist.github.com/67252933bcfea50c996b44dd20225962#file-asyncapi-yml"&gt;example in YAML&lt;/a&gt; can be found on GitHub. This example describes an event with the title &lt;code&gt;User signed-up event&lt;/code&gt;, published to the &lt;code&gt;user/signedup&lt;/code&gt; channel. These events have three properties: &lt;code&gt;fullName&lt;/code&gt;, &lt;code&gt;email&lt;/code&gt;, and &lt;code&gt;age&lt;/code&gt; . Each property is defined using semantics from &lt;a href="https://json-schema.org/"&gt;JSON Schema&lt;/a&gt;. Although it's not shown in this example, AsyncAPI also allows you to specify event headers and whether these events will be available through different protocol bindings such as &lt;a href="https://kafka.apache.org/"&gt;Kafka&lt;/a&gt;, &lt;a href="https://www.amqp.org/"&gt;AMQP&lt;/a&gt;, &lt;a href="https://mqtt.org/"&gt;MQTT&lt;/a&gt;, or &lt;a href="https://www.w3.org/TR/websockets/"&gt;WebSocket&lt;/a&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;asyncapi: 2.0.0 id: urn:com.asyncapi.examples.user info: title: User signed-up event version: 0.1.1 channels: user/signedup: publish: message: payload: type: object properties: fullName: type: string email: type: string format: email age: type: integer minimum: 18&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;CloudEvents with AsyncAPI&lt;/h2&gt; &lt;p&gt;The explanations and examples I've shown reveal that the CloudEvents with AsyncAPI standards tackle different scopes. Thus they do not have to be treated as mutually exclusive. You can actually combine them to achieve a complete event specification, including application definition, channels description, structured envelope, and detailed functional data carried by the event.&lt;/p&gt; &lt;p&gt;The general idea behind the combination is to use an AsyncAPI specification as a hosting document. It holds references to CloudEvents attributes and adds more details about the event format.&lt;/p&gt; &lt;p&gt;You can use two mechanisms in AsyncAPI to ensure this combination. Choosing the correct mechanism might depend on the protocol you choose to convey your events. Things aren't perfect yet and you'll have to make a choice.&lt;/p&gt; &lt;p&gt;Let's take the example of using &lt;a href="https://developers.redhat.com/topics/kafka-kubernetes"&gt;Apache Kafka&lt;/a&gt; to distribute events:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;In the structured content mode, CloudEvents meta-information is tangled up with the &lt;code&gt;data&lt;/code&gt; in the messages value. For that mode, we'll use the JSON Schema composition mechanism that is accessible from AsyncAPI.&lt;/li&gt; &lt;li&gt;In the binary content mode (for which we can use &lt;a href="https://avro.apache.org/"&gt;Avro&lt;/a&gt;), meta-information for each event is dissociated from the message value and inserted, instead, into the header of each message. For that, we'll use the &lt;a href="https://www.asyncapi.com/docs/specifications/2.0.0#messageTraitObject"&gt;MessageTrait&lt;/a&gt; application mechanism present in AsyncAPI.&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;Structured content mode&lt;/h3&gt; &lt;p&gt;This section rewrites the previous AsyncAPI example to use CloudEvents in structured content mode. The resulting &lt;a href="https://gist.github.com/035ccc4d7b7cdd414f0ebc5a53e80c4c#file-asyncapi-ce-yaml"&gt;definition&lt;/a&gt; contains the following elements worth noting:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;The definition of &lt;code&gt;headers&lt;/code&gt; that starts on line 16 contains our application's &lt;code&gt;custom-header&lt;/code&gt;, as well as the mandatory CloudEvents &lt;code&gt;content-type&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;A &lt;code&gt;schemas&lt;/code&gt; field refers to the CloudEvents specification on line 33, re-using this specification as a basis for our message.&lt;/li&gt; &lt;li&gt;The &lt;code&gt;schemas&lt;/code&gt; field also refers to a refined version of the &lt;code&gt;data&lt;/code&gt; property description on line 36.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Here is the definition:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;asyncapi: '2.0.0' id: 'urn:io.microcks.example.user-signedup' info: title: User signed-up CloudEvents API structured version: 0.1.3 defaultContentType: application/json channels: user/signedup: subscribe: message: bindings: kafka: key: type: string description: Timestamp of event as milliseconds since 1st Jan 1970 headers: type: object properties: custom-header: type: string content-type: type: string enum: - 'application/cloudevents+json; charset=UTF-8' payload: $ref: '#/components/schemas/userSignedUpPayload' examples: [...] components: schemas: userSignedUpPayload: type: object allOf: - $ref: 'https://raw.githubusercontent.com/cloudevents/spec/v1.0.1/spec.json' properties: data: $ref: '#/components/schemas/userSignedUpData' userSignedUpData: type: object properties: fullName: type: string email: type: string format: email age: type: integer minimum: 18&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Binary content mode&lt;/h3&gt; &lt;p&gt;Now, we'll apply the binary content mode to the AsyncAPI format. The resulting &lt;a href="https://gist.github.com/d5eca1c76fd57e5b3326b5d5db26bbd3#file-asyncapi-ce-yaml"&gt;definition&lt;/a&gt; shows that event properties have moved out of this format. Other important things to notice here are:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;A trait is applied at the message level on line 16. The trait resource is a partial AsyncAPI document containing a &lt;code&gt;MessageTrait&lt;/code&gt; definition. This trait will bring in all the mandatory attributes (&lt;code&gt;ce_*&lt;/code&gt;) from CloudEvents. It is the equivalent of the CloudEvents JSON Schema.&lt;/li&gt; &lt;li&gt;This time we're specifying our event payload using an Avro schema as specified on line 25.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Here is the definition:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;asyncapi: '2.0.0' id: 'urn:io.microcks.example.user-signedup' info: title: User signed-up CloudEvents API binary version: 0.1.3 channels: user/signedup: subscribe: message: bindings: kafka: key: type: string description: Timestamp of event as milliseconds since 1st Jan 1970 traits: - $ref: 'https://raw.githubusercontent.com/microcks/microcks-quickstarters/main/cloud/cloudevents/cloudevents-v1.0.1-asyncapi-trait.yml' headers: type: object properties: custom-header: type: string contentType: avro/binary schemaFormat: application/vnd.apache.avro+json;version=1.9.0 payload: $ref: './user-signedup.avsc#/User' examples: [...] &lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;What are the benefits of combining CloudEvents with AsyncAPI?&lt;/h3&gt; &lt;p&gt;Whichever content mode you chose, you now have a comprehensive description of your event and all the elements of your event-driven architecture. The description guarantees the low-level interoperability of the CloudEvents-plus-AsyncAPI combination, along with the ability to be routed and trigger a function in a serverless world. In addition, you provide a complete description of the carried &lt;code&gt;data&lt;/code&gt; that will be of great help for applications consuming and processing events.&lt;/p&gt; &lt;h2&gt;Simulating CloudEvents with Microcks&lt;/h2&gt; &lt;p&gt;Let's tackle the second challenge stated at the beginning of this article: How can developers efficiently work as a team without having to wait for someone else's events? We've seen how to fully describe events. However, it would be even better to have a pragmatic approach for leveraging this CloudEvents-plus-AsyncAPI contract. That's where &lt;a href="https://microcks.io/"&gt;Microcks&lt;/a&gt; comes to the rescue.&lt;/p&gt; &lt;h3&gt;What is Microcks?&lt;/h3&gt; &lt;p&gt;Microcks is an &lt;a href="https://developers.redhat.com/topics/open-source"&gt;open source&lt;/a&gt; &lt;a href="https://developers.redhat.com/topics/kubernetes/"&gt;Kubernetes&lt;/a&gt;-native tool for mocking/simulating and testing APIs. One purpose of Microcks is to turn your API contract (such as OpenAPI, AsyncAPI, or the &lt;a href="https://getpostman.com/"&gt;Postman&lt;/a&gt; collection) into live mocks in seconds. Once it has imported your AsyncAPI contract, Microcks starts producing mock events on a message broker at a defined frequency.&lt;/p&gt; &lt;p&gt;Using Microcks you can simulate CloudEvents in seconds, without writing a single line of code. Microcks allows the team that is relying on input events to immediately start working. They do not have to wait for the team that is coding the application that will publicize events.&lt;/p&gt; &lt;h3&gt;Using Microcks for CloudEvents&lt;/h3&gt; &lt;p&gt;To produce CloudEvents events with Microcks, simply re-use examples by adding them to your contract. We omitted the &lt;code&gt;examples&lt;/code&gt; property before, but we'll now add that property to our &lt;a href="https://gist.github.com/820c925b8ff84929ebf0c30ad1900c62#file-asyncapi-ce-yaml"&gt;example in binary content mode&lt;/a&gt; on line 26:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;asyncapi: '2.0.0' id: 'urn:io.microcks.example.user-signedup' info: title: User signed-up CloudEvents API binary version: 0.1.3 channels: user/signedup: subscribe: message: bindings: kafka: key: type: string description: Timestamp of event as milliseconds since 1st Jan 1970 traits: - $ref: 'https://raw.githubusercontent.com/microcks/microcks-quickstarters/main/cloud/cloudevents/cloudevents-v1.0.1-asyncapi-trait.yml' headers: type: object properties: custom-header: type: string contentType: avro/binary schemaFormat: application/vnd.apache.avro+json;version=1.9.0 payload: $ref: './user-signedup.avsc#/User' examples: - john: summary: Example for John Doe user headers: ce_specversion: "1.0" ce_type: "io.microcks.example.user-signedup" ce_source: "/mycontext/subcontext" ce_id: "{{uuid()}}" ce_time: "{{now(yyyy-MM-dd'T'HH:mm:SS'Z')}}" content-type: application/avro sentAt: "2020-03-11T08:03:38Z" payload: fullName: John Doe email: john@microcks.io age: 36&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Key points to note are:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;You can put in as many examples as you want because this property becomes a map in AsyncAPI.&lt;/li&gt; &lt;li&gt;You can specify both &lt;code&gt;headers&lt;/code&gt; and &lt;code&gt;payload&lt;/code&gt; values.&lt;/li&gt; &lt;li&gt;Even if &lt;code&gt;payload&lt;/code&gt; will be Avro-binary encoded, you use YAML or JSON to specify examples.&lt;/li&gt; &lt;li&gt;You can use templating functions through the &lt;code&gt;{{ }}&lt;/code&gt; notation to introduce &lt;a href="https://microcks.io/documentation/using/advanced/templates/#function-expressions"&gt;random or dynamic values&lt;/a&gt;.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Once the schema is imported into Microcks, it discovers the API definition as well as the different examples. Microcks starts immediately producing mock events on the Kafka broker it is connected to—every three seconds in our example (see Figure 1).&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/rh-simulating-cloud-events_0.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/rh-simulating-cloud-events_0.png?itok=uI0qn302" width="600" height="327" alt="A Microcks import of AsyncAPI with CloudEvents." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: A Microcks import of AsyncAPI with CloudEvents. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Since release &lt;a href="https://microcks.io/blog/microcks-1.2.0-release/"&gt;1.2.0&lt;/a&gt;, Microcks also supports the connection to a schema registry. Therefore, it publishes the Avro schema used at the mock-message publication time. You can use the &lt;a href="https://github.com/edenhill/kafkacat"&gt;kafkacat&lt;/a&gt; command-line interface (CLI) tool to connect to the Kafka broker and registry, and then inspect the content of mock events. Here we're using the &lt;a href="https://www.apicur.io/registry/"&gt;Apicurio service registry&lt;/a&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ kafkacat -b my-cluster-kafka-bootstrap.apps.try.microcks.io:9092 -t UsersignedupCloudEventsAPI_0.1.3_user-signedup -s value=avro -r http://apicurio-registry.apps.try.microcks.io/api/ccompat -o end -f 'Headers: %h - Value: %s\n' --- OUTPUT % Auto-selecting Consumer mode (use -P or -C to override) % Reached end of topic UsersignedupCloudEventsAPI_0.1.3_user-signedup [0] at offset 276 Headers: sentAt=2020-03-11T08:03:38Z,content-type=application/avro,ce_id=7a8cc388-5bfb-42f7-8361-0efb4ce75c20,ce_type=io.microcks.example.user-signedup,ce_specversion=1.0,ce_time=2021-03-09T15:17:762Z,ce_source=/mycontext/subcontext - Value: {"fullName": "John Doe", "email": "john@microcks.io", "age": 36} % Reached end of topic UsersignedupCloudEventsAPI_0.1.3_user-signedup [0] at offset 277 Headers: ce_id=dde8aa04-2591-4144-aa5b-f0608612b8c5,sentAt=2020-03-11T08:03:38Z,content-type=application/avro,ce_time=2021-03-09T15:17:733Z,ce_type=io.microcks.example.user-signedup,ce_specversion=1.0,ce_source=/mycontext/subcontext - Value: {"fullName": "John Doe", "email": "john@microcks.io", "age": 36} % Reached end of topic UsersignedupCloudEventsAPI_0.1.3_user-signedup [0] at offset 279&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;We can check that the emitted events respect both the CloudEvents meta-information structure and the AsyncAPI &lt;code&gt;data&lt;/code&gt; definition. Moreover, each event has different random attributes, which allows it to simulate diversity and variation for the consuming application.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;This article has shown how to solve some of the challenges that come with an event-driven architecture.&lt;/p&gt; &lt;p&gt;First, I described how recent standards, CloudEvents and AsyncAPI, focus on different scopes: the event for CloudEvents and the application for AsyncAPI.&lt;/p&gt; &lt;p&gt;Then I demonstrated how to combine the specifications to provide a comprehensive description of all the elements involved in an event-driven architecture: application definition, channels description, structured envelope, and detailed functional data carried by the event. The specifications are complementary, so you can use one or both depending on how deep you want to go in your formal description.&lt;/p&gt; &lt;p&gt;Finally, you've seen how to use Microcks to simulate any events based on AsyncAPI, including those generated by CloudEvents, just by using examples. Microcks answers the challenge of working, testing, and validating autonomously when different development teams are using an event-driven architecture.&lt;/p&gt; &lt;p&gt;I hope you learned something new—if so, please consider reacting, commenting, or sharing. Thanks for reading.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/06/02/simulating-cloudevents-asyncapi-and-microcks" title="Simulating CloudEvents with AsyncAPI and Microcks"&gt;Simulating CloudEvents with AsyncAPI and Microcks&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/CKaov6paM0c" height="1" width="1" alt=""/&gt;</summary><dc:creator>Laurent Broudoux</dc:creator><dc:date>2021-06-02T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/06/02/simulating-cloudevents-asyncapi-and-microcks</feedburner:origLink></entry><entry><title type="html">Custom Layered Immutable Spring Boot Kie Server</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/hYH2jJwUYUQ/custom-layered-immutable-spring-boot-kie-server.html" /><author><name>Gonzalo Muñoz Fernández</name></author><id>https://blog.kie.org/2021/06/custom-layered-immutable-spring-boot-kie-server.html</id><updated>2021-06-01T23:06:08Z</updated><content type="html">“Uncommon thinkers reuse what common thinkers refuse” (J.R.D. Tata) When creating an image for an immutable Spring Boot Kie Server application, it is possible to split up the files and folders belonging to the fat-jar into different layers. Main advantages of this approach are: 1. Libraries, code, and resources are grouped into layers based on the likelihood to change between builds. This reduces the image generation time. 2. Layers downloaded once (saving disk space and bandwidth) and reused for other images. 3. Execution over the unzipped classes is a little bit faster than launching the fat-jar: java -jar app.jar Considering all these points, it is completely meaningful to layer the immutable Spring Boot Kie Server -isolating the KJARs into a new custom layer- as business assets in KJARs are more likely to change. PACKAGING SPRING BOOT KIE SERVER WITH LAYERS The spring-boot-maven-plugin is in charge of creating the immutable fat-jar containing all the KJAR files and their dependencies. For triggering this process, just add the following properties to the Spring Boot application.properties file: kieserver.classPathContainer=true kieserver.autoScanDeployments=true Next, we have to enable the layers into the pom.xml, pointing out to the configuration file layers.xml where we define how the folders, files, and resources are separated into different layers and the order of them. &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;version&gt;${springboot.version}&lt;/version&gt; &lt;configuration&gt; &lt;layers&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;configuration&gt;${project.basedir}/src/layers.xml&lt;/configuration&gt; &lt;/layers&gt; &lt;image&gt; &lt;name&gt;${spring-boot.build-image.name}&lt;/name&gt; &lt;/image&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;goals&gt; &lt;goal&gt;repackage&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; In this layers.xml file, we define the following layers in this order (the first four are default ones, adding custom kjars layer at the end, as it is the more likely to change during application lifetime): * dependencies any dependency whose version does not contain SNAPSHOT. * spring-boot-loader for the loader classes. * snapshot-dependencies dependencies whose version contains SNAPSHOT. * application for local module dependencies, application classes, and resources but KJARs. * kjars for all the KJARs in the BOOT-INF/classes/KIE-INF folder, which were separated during package-dependencies-kjar goal execution. &lt;layers xmlns="http://www.springframework.org/schema/boot/layers" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.springframework.org/schema/boot/layers https://www.springframework.org/schema/boot/layers/layers-2.5.xsd"&gt; &lt;application&gt; &lt;into layer="spring-boot-loader"&gt; &lt;include&gt;org/springframework/boot/loader/**&lt;/include&gt; &lt;/into&gt; &lt;into layer="kjars"&gt; &lt;include&gt;BOOT-INF/classes/KIE-INF/**&lt;/include&gt; &lt;/into&gt; &lt;into layer="application" /&gt; &lt;/application&gt; &lt;dependencies&gt; &lt;into layer="snapshot-dependencies"&gt; &lt;include&gt;*:*:*SNAPSHOT&lt;/include&gt; &lt;/into&gt; &lt;into layer="dependencies"&gt; &lt;includeModuleDependencies /&gt; &lt;/into&gt; &lt;/dependencies&gt; &lt;layerOrder&gt; &lt;layer&gt;dependencies&lt;/layer&gt; &lt;layer&gt;spring-boot-loader&lt;/layer&gt; &lt;layer&gt;snapshot-dependencies&lt;/layer&gt; &lt;layer&gt;application&lt;/layer&gt; &lt;layer&gt;kjars&lt;/layer&gt; &lt;/layerOrder&gt; &lt;/layers&gt; Finally, once we have the configuration of the pom.xml and layers.xml set up, we may launch the package process by invoking to the maven command: $ mvn clean package -DskipTests SPRING BOOT KIE SERVER LAYERS INSPECT We can check out the result of this layering process using the property jarmode=layertools with the list argument for the generated fat-jar: $ java -Djarmode=layertools -jar target/immutable-springboot-kie-server-1.0.0.jar list dependencies spring-boot-loader snapshot-dependencies application kjars Our custom kjars layer is the last one as defined in the layerOrder node of layers.xml file. Another interesting file we may check out is the layers.idx where packaging information (separated folders and layer order) is stored. $ cat application/BOOT-INF/layers.idx - "dependencies": - "BOOT-INF/lib/" - "spring-boot-loader": - "org/" - "snapshot-dependencies": - "application": - "BOOT-INF/classes/application.properties" - "BOOT-INF/classes/org/" - "BOOT-INF/classes/quartz-db.properties" - "BOOT-INF/classpath.idx" - "BOOT-INF/layers.idx" - "META-INF/" - "kjars": - "BOOT-INF/classes/KIE-INF/" Moreover, we can extract the layers again using the property jarmode=layertools with the extract argument for the generated fat-jar: $ java -Djarmode=layertools -jar target/immutable-springboot-kie-server-1.0.0.jar extract $ tree kjars kjars └── BOOT-INF └── classes └── KIE-INF └── lib ├── kjar-sample-1.0.0.jar ├── kjar-sample-1.1.0.jar └── other-kjar-1.0.0.jar 4 directories, 3 files We can use this utility in the Dockerfile for easily extracting the layers of the fat-jar and dockerize our immutable Spring Boot Kie Server application. LAYERED DOCKERFILE We can define a multi-stage Dockerfile to take advantage of this layering and build the image: FROM openjdk:8-slim as builder WORKDIR application ARG JAR_FILE=target/*.jar COPY ${JAR_FILE} immutable-springboot-kie-server-1.0.0.jar RUN java -Djarmode=layertools -jar immutable-springboot-kie-server-1.0.0.jar extract FROM openjdk:8-slim WORKDIR application COPY --from=builder application/dependencies/ ./ COPY --from=builder application/spring-boot-loader/ ./ COPY --from=builder application/snapshot-dependencies/ ./ COPY --from=builder application/application/ ./ COPY --from=builder application/kjars/ ./ ENTRYPOINT ["java", "org.springframework.boot.loader.JarLauncher"] TIP: You may consider any other layering depending on the likelihood of changes for the grouped libraries, code, and resources. Happy layering!! The post appeared first on .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/hYH2jJwUYUQ" height="1" width="1" alt=""/&gt;</content><dc:creator>Gonzalo Muñoz Fernández</dc:creator><feedburner:origLink>https://blog.kie.org/2021/06/custom-layered-immutable-spring-boot-kie-server.html</feedburner:origLink></entry></feed>
